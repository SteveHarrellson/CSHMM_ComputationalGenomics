{"cells":[{"cell_type":"markdown","id":"f248ddf3","metadata":{"id":"f248ddf3"},"source":["# Prediciting Q3 protein secondary structure using HMM with pre-processed emission sequence\n","\n","Two functions have been coded to pre-process the emission sequence. Using this code, one can either use the lookback encoder to mimic an alpha helix interaction of the amino acids, or the palindromic encoder for a beta sheet modelling."]},{"cell_type":"code","execution_count":null,"id":"aca0128a","metadata":{"id":"aca0128a"},"outputs":[],"source":["import gzip\n","import numpy as np\n","import pandas as pd\n","import pickle\n","import time\n","from sklearn.model_selection import train_test_split\n","import random"]},{"cell_type":"code","execution_count":null,"id":"abc50fe5","metadata":{"id":"abc50fe5"},"outputs":[],"source":["def get_data(arr, residue_list, q8_list, columns, r, f, bounds=None):\n","    \n","    \"\"\"\n","    This function retrieves and formats data from the CB6133_filtered and CB531 datasets [1][2]\n","    Codes is slighlty modified from code provided by [3][4]\n","    \n","    [1] Jian Zhou and Olga G. Troyanskaya. Deep supervised and convolutional generative stochastic network for\n","        protein s\n","    [2] Jian Zhou and Olga G. Troyanskaya. CB6133 dataset.\n","        https://www.princeton.edu/~jzthree/datasets/ICML2014/dataset_readme.txt, 2014.\n","    [3] Iddo Drori et al. High Quality Prediction of Protein Q8 Secondary Structure by\n","        Diverse Neural Network Architectures. arXiv preprint arXiv:1811.07143, 2018\n","    [4] https://github.com/idrori/cu-ssp/blob/master/model_1/model_1.py\n","    \"\"\"\n","    \n","    if bounds is None: bounds = range(len(arr))\n","    \n","    data = [None for i in bounds]\n","    for i in bounds:\n","        seq, q8, q3, q2, profiles = '', '', '', '', []\n","        for j in range(r):\n","            jf = j*f\n","            \n","            # Residue convert from one-hot to decoded\n","            residue_onehot = arr[i,jf+0:jf+22]\n","            residue = residue_list[np.argmax(residue_onehot)]\n","\n","            # Q8 one-hot encoded to decoded structure symbol\n","            residue_q8_onehot = arr[i,jf+22:jf+31]\n","            residue_q8 = q8_list[np.argmax(residue_q8_onehot)]\n","\n","            if residue == 'NoSeq': break      # terminating sequence symbol\n","\n","            nc_terminals = arr[i,jf+31:jf+33] # nc_terminals = [0. 0.]\n","            sa = arr[i,jf+33:jf+35]           # sa = [0. 0.]\n","            profile = arr[i,jf+35:jf+57]      # profile features\n","            \n","            seq += residue # concat residues into amino acid sequence\n","\n","            #encode q3 structure\n","            if residue_q8 in 'GHI':\n","                q3 += 'H'\n","                q2 += 'A'\n","            elif residue_q8 in 'TBSL':\n","                q3 += 'C'\n","                q2 += 'X'\n","            elif residue_q8 in 'E':\n","                q3 += 'E'\n","                q2 += 'X'\n","            else:\n","                q3 += 'Z'\n","                q2 += 'Z'\n","            \n","            q8  += residue_q8 # concat secondary structure into secondary structure sequence\n","            profiles.append(profile)\n","        \n","        data[i] = [str(i+1), len(seq), seq, np.array(profiles), q8, q3, q2]\n","    \n","    return pd.DataFrame(data, columns=columns)\n","\n","\n","def encode_sequence(sequence, code):\n","    \n","    \"\"\"\n","    Provided an input sequence and a code, returns the encoding of the sequence\n","    \"\"\"\n","    \n","    encoded_seq = []\n","    \n","    for x in sequence:\n","        try:\n","            idx = code[x]\n","            encoded_seq.append(idx)\n","        except Exception as e:\n","            print(f\"Error: {e}\")\n","            break\n","    \n","    return encoded_seq\n","\n","\n","def format_dataset(df, emission_code, state_code, exp_col=\"q3_expected\"):\n","    \n","    \"\"\"\n","    Provided a dataframe which contains the amino sequences and the hidden sequence,\n","    this function encodes those sequences according to the provided codes\n","    and return them\n","    \n","    *exp_col specifies if want to encode the q8, q3, or q2 hidden sequence\n","    \"\"\"\n","    \n","    assert ('id' in df.columns and 'len' in df.columns and 'input' in df.columns and exp_col in df.columns)\n","    \n","    formattedDF = pd.DataFrame(columns=['id','len','input','expected'])\n","\n","    for i in range(len(df)):\n","        \n","        sid = df.iloc[i].id\n","        slen = df.iloc[i].len\n","        enc_input = encode_sequence(df.iloc[i].input, emission_code)\n","        enc_expected = encode_sequence(df.iloc[i][exp_col], state_code)\n","        \n","        assert (len(enc_input) == len(enc_expected))\n","        \n","        formattedDF = formattedDF.append({'id':sid, 'len':slen, 'input':enc_input, 'expected':enc_expected}, ignore_index=True)\n","\n","    return formattedDF\n","\n","def estimate_transition_matrix(df, state_code):\n","    \"\"\"\n","    Given a dataframe that has the data for the amino sequences and their corresponding hidden sequence,\n","    we use the data to compute the MLEs of the emission probablities.\n","    \n","    ex. estimated P(emission=A|state=H) = count(emission=A,state=H) / sum_over_all_emission count(emission, state=H)\n","    \n","    *implemented a pseudocount of +1 for cases where we have 0 observations of a certain (emission,state) combo\n","    \"\"\"\n","    \n","    n_states = len(state_code)\n","    \n","    #using pseudocount of +1\n","    counts = np.ones(shape=(n_states, n_states), dtype=float)\n","    \n","    for i in range(len(df)):\n","        \n","        state_seq = df.iloc[i].expected\n","        seq_len = len(df.iloc[i].expected)\n","        \n","        for j in range(seq_len - 1):\n","            \n","            x = state_seq[j]\n","            y = state_seq[j+1]\n","            \n","            counts[x,y] += 1\n","    \n","    #transform counts to probability by normalizing of row sums\n","    row_sums = np.sum(counts, axis=1)\n","    T = counts / row_sums.reshape((-1,1))\n","    \n","    return T\n","\n","\n","def estimate_emission_matrix(df, state_code, emission_code):\n","    \"\"\"\n","    Given a dataframe that has the data for the amino sequences and their corresponding hidden sequence,\n","    we use the data to compute the MLEs of the transition probablities.\n","    \n","    ex. estimated P(state_{t+1}=E|state_{t}=H) = count(state_{t}=H, state_{t+1}=E) / sum_over_all_states count(state_{t}=H, state_{t+1})\n","    \n","    *implemented a pseudocount of +1 for cases where we have 0 observations of a certain (state,state) combo\n","    \"\"\"\n","    \n","    n_states = len(state_code)\n","    n_emissions = len(emission_code)\n","    \n","    #using pseudocount of +1\n","    #Steve Contribution\n","    #Change counts matrix to be n_states x n_emissions x n_emissions\n","    #Each state has a n_emissions x n_emissions context-dependent matrix associated with it\n","    counts = np.ones(shape=(n_states, n_emissions, n_emissions), dtype=float)\n","    \n","    for i in range(len(df)):\n","        \n","        state_seq = df.iloc[i].expected\n","        #emission_seq now of form: [(1,4),(2,5),...]\n","        #store context in y\n","        #store new state in z\n","        emission_seq = df.iloc[i].input\n","        #df['len'] no longer reflects true length\n","        seq_len = len(df.iloc[i].input)\n","        \n","        for j in range(seq_len):\n","            \n","            x = state_seq[j]\n","            y = emission_seq[j][0]\n","            z = emission_seq[j][1]\n","            \n","            counts[x,y,z] += 1\n","\n","    #transform counts to probability by normalizing of row sums\n","    #not sure how pendo's way works, implementing my own here\n","    #index y is 'context', normalize by this row\n","    for i in range(counts.shape[0]):\n","        for j in range(counts.shape[1]):\n","            counts[i,j,:] = counts[i,j,:]/np.sum(counts[i,j,:])\n","    \n","    E = counts\n","    \n","    #row_sums = np.sum(counts, axis=1)\n","    #print(row_sums.shape)\n","    #E = counts / row_sums.reshape((-1,3))\n","\n","    return E\n","\n","def start_distribution(df,state_code):\n","    \"\"\"\n","    Given a dataframe that has the data for the amino sequences and their corresponding hidden sequence,\n","    we use the data to compute the MLEs of the start distribution.\n","    \n","    ex. estimated P(state=H) = count(state=H) / sum_over_all_states count(state)\n","    \n","    *implemented a pseudocount of +1 for cases where we have 0 observations of a certain state\n","    \"\"\"\n","    \n","    n_states = len(state_code)\n","    \n","    #using pseudocount of +1\n","    counts = np.array([1.0] * n_states)\n","    \n","    for i in range(len(df)):\n","        \n","        state_seq = df.iloc[i].expected\n","        seq_len = len(df.iloc[i].expected)\n","        \n","        for j in range(seq_len):\n","            \n","            x = state_seq[j]\n","            \n","            counts[x] += 1\n","    \n","    #transform counts to probability by normalizing of row sums\n","    total = sum(counts)\n","    pi = counts / total\n","    \n","    return pi\n","\n","def viterbi_decoding(T,E,pi,seq):\n","    \"\"\"\n","    This functions performs viterbi decoding to get the predicted hidden sequence,\n","    given the input emission sequence as well as the transition matrix, emission matrix,\n","    and the start distribution\n","    \"\"\"\n","    \n","    #sequence length\n","    N = len(seq)\n","    #num of states\n","    M = T.shape[0]\n","    \n","    assert (M == len(pi))\n","    \n","    #V will store viterbi values\n","    V = np.zeros(shape=(M, N), dtype=float)\n","    \n","    #P will store prev state from which we transitioned into state m and time n to achieve the max value of V[m,n]\n","    #(i.e. pointer to help us reconstruct sequence after predicting most probable path in viterbi graph)\n","    P = np.empty(shape=(M, N))\n","    P[:] = np.NaN\n","    \n","    #populate viterbi matrix\n","    for n in range(N):\n","        \n","        #get current emissions\n","        e = seq[n]\n","        \n","        for m in range(M):\n","            \n","            #initilize viterbi value for current timestep given state m to be -infty\n","            maxV = float(\"-inf\")\n","            prev = np.NaN\n","            \n","            #get log prob. of emission given state m\n","            emiss_logp = np.log(E[m,e[0],e[1]])\n","            \n","            #start of sequence\n","            if n == 0:\n","                start_logp = np.log(pi[m])\n","                maxV = emiss_logp + start_logp\n","                \n","            else:\n","            \n","                #solve for max value for V[m,n]\n","                for i in range(M): \n","\n","                    #get previous timestep viterbi value for state i (which should be a log prob)\n","                    prev_vit = V[i, n-1]\n","\n","                    #get log prob of transition from state i to m\n","                    trans_logp = np.log(T[i,m])\n","\n","                    #update viterbi value for current timestep given state m\n","                    curV = prev_vit + trans_logp + emiss_logp\n","\n","                    if curV > maxV:\n","                        maxV = curV\n","                        prev = i\n","        \n","            V[m,n] = maxV\n","            P[m,n] = prev\n","    \n","    #initialize with state with highest probability at end of sequence\n","    best_path = [np.argmax(V[:,-1])]\n","    \n","    #work backwards to reconstruct sequence\n","    for n in range(N-1,0,-1):\n","        \n","        #determine where we are\n","        cur_state = best_path[0]\n","\n","        #find state from which we came that yielded highest probability to current state at current time\n","        prev_state = int(P[cur_state,n])\n","\n","        #prepend previous state\n","        best_path = [prev_state] + best_path\n","    \n","    return best_path\n","\n","\n","def getPredictions(df, T, E, pi):\n","    \n","    \"\"\"\n","    get predictions for all sequences in specified dataset using provided HMM\n","    \"\"\"\n","    \n","    results = pd.DataFrame(columns=['input','predicted','expected'])\n","    \n","    for i in range(len(df)):\n","        \n","        #change amino_seq input\n","        amino_seq = df.iloc[i].input\n","        pred_seq = viterbi_decoding(T,E,pi,amino_seq)\n","        exp_seq = df.iloc[i].expected\n","        \n","        #Steve commented this out because we are padding our start with new symbols\n","        #assert(len(amino_seq) == len(pred_seq) and len(amino_seq) == len(exp_seq))\n","        \n","        results = results.append({'input':amino_seq, 'predicted':pred_seq, 'expected':exp_seq}, ignore_index=True)\n","    \n","    return results\n","\n","def HMMaccuracy(df,q=3):\n","    \n","    \"\"\"\n","    Compute accurcay of HMM given a dataframe the has the input emission sequences,\n","    the predicted hidden sequences, and the actual hidden sequences\n","    \n","    *q specifies whether the predicion was made for q2, q3, or q8 protein structure\n","    \"\"\"\n","    \n","    #row represents expected state\n","    #col represents predicted state\n","    counts = np.zeros(shape=(q,q), dtype=int)\n","    \n","    for i in range(len(df)):\n","\n","        #get predicted and expected hidden sequence from dataframe\n","        pred = df.iloc[i].predicted\n","        exp = df.iloc[i].expected\n","        \n","        #assert (len(pred) == len(exp))\n","        \n","        for j in range(len(pred)):\n","            \n","            x = exp[j]\n","            y = pred[j]\n","            counts[x,y] += 1\n","    \n","    rowSum = np.sum(counts, axis=1)\n","    colSum = np.sum(counts, axis=0)\n","    \n","    #true positive (negative) / total predicted positive (negative)\n","    precision = np.array([counts[i,i] / colSum[i] for i in range(q)])\n","    \n","    #true positive (negative) / total actual positive (negative)\n","    recall = np.array([counts[i,i] / rowSum[i] for i in range(q)])\n","    \n","    accuracy = 0\n","    for i in range(q):\n","        accuracy += counts[i,i]\n","    accuracy = accuracy / sum(rowSum)\n","    \n","    \n","    return accuracy, precision, recall, counts\n","\n","#Pre-processing functions for emissions\n","\n","def encode_lookback_context(sequence,spacing = 4):\n","  '''Encode context by keeping track of the i-spacing residue'''\n","  encoded_sequence = []\n","  for i in range(spacing,len(sequence)):\n","      encoded_sequence.append((sequence[i-spacing],sequence[i]))\n","  return encoded_sequence\n","\n","def encode_palindrome_context(sequence, length):\n","  '''\n","  Encode context by associating amino acids around a turn together. This function supposes that each strand is of fixed length, and then turns.\n","  '''\n","  encoded_sequence = []\n","  for i in range(length,len(sequence)):\n","    encoded_sequence.append((sequence[i-2*(i%length)],sequence[i]))\n","  return encoded_sequence\n","\n"]},{"cell_type":"code","source":["l = range(1,20)\n","#Example of lookback encoding\n","print(encode_lookback_context(l))\n","\n","#Example of palindrome encoding. \n","#There is a turn every 5 amino acids. The second number of the tuple is always increasing at a constant rate.\n","print(encode_palindrome_context(l,5))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dFJdc4ZtGvnt","executionInfo":{"status":"ok","timestamp":1651685418905,"user_tz":240,"elapsed":288,"user":{"displayName":"Mujahed Darwaza","userId":"06409676498706762728"}},"outputId":"d02dca6d-f973-4e64-f618-7057bde5d694"},"id":"dFJdc4ZtGvnt","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[(1, 5), (2, 6), (3, 7), (4, 8), (5, 9), (6, 10), (7, 11), (8, 12), (9, 13), (10, 14), (11, 15), (12, 16), (13, 17), (14, 18), (15, 19)]\n","[(6, 6), (5, 7), (4, 8), (3, 9), (2, 10), (11, 11), (10, 12), (9, 13), (8, 14), (7, 15), (16, 16), (15, 17), (14, 18), (13, 19)]\n"]}]},{"cell_type":"markdown","id":"984ee1c9","metadata":{"id":"984ee1c9"},"source":["Step 1: Load data"]},{"cell_type":"code","execution_count":null,"id":"a55243b5","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a55243b5","executionInfo":{"status":"ok","timestamp":1651429769117,"user_tz":240,"elapsed":12430,"user":{"displayName":"Mujahed Darwaza","userId":"06409676498706762728"}},"outputId":"b6c09a2e-891a-4459-8f22-e28842533e50"},"outputs":[{"output_type":"stream","name":"stdout","text":["Data Loaded\n","CB6133 shape: (5365, 39900)\n","CB513 shape: (514, 39900)\n"]}],"source":["#seed so get consistent results for every run\n","random.seed(0)\n","\n","cb513 = np.load('cb513+profile_split1.npy.gz')\n","cb6133filtered = np.load('cullpdb+profile_5926_filtered.npy.gz')\n","print(\"Data Loaded\")\n","print(f\"CB6133 shape: {cb6133filtered.shape}\")\n","print(f\"CB513 shape: {cb513.shape}\")"]},{"cell_type":"markdown","id":"c910596f","metadata":{"id":"c910596f"},"source":["### Step 2: Process Data"]},{"cell_type":"code","execution_count":null,"id":"091dae2b","metadata":{"id":"091dae2b","outputId":"9f71003e-251e-40df-8b06-819669f7df82","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651429778981,"user_tz":240,"elapsed":9877,"user":{"displayName":"Mujahed Darwaza","userId":"06409676498706762728"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Turning data arrays into dataframes\n"]}],"source":["maxlen_seq = r = 700 # protein residues padded to 700\n","f = 57  # number of features for each residue\n","\n","residue_list = list('ARNDCEQGHILKMFPSTWYVX') + ['NoSeq']\n","q8_list      = list('LBEGIHST') + ['NoSeq']\n","q3_list      = list('HCE') + ['NoSeq']\n","q2_list      = list('AX') + ['NoSeq']\n","\n","columns = [\"id\", \"len\", \"input\", \"profiles\", \"q8_expected\", \"q3_expected\", \"q2_expected\"]\n","\n","print(\"Turning data arrays into dataframes\")\n","\n","# train, dev, test split\n","# break out 10% of train data to be used as dev set\n","train_df, dev_df = train_test_split(get_data(cb6133filtered, residue_list, q8_list, columns, r, f), test_size=0.1)\n","test_df  = get_data(cb513, residue_list, q8_list, columns, r, f)"]},{"cell_type":"markdown","id":"73751055","metadata":{"id":"73751055"},"source":["### Step 3: Encode Sequences and Format DataFrames\n","    (a) Create codes to encode emission and hidden sequences\n","    (b) Apply encodings & specify hidden sequence of interest (q2, q3, q8)"]},{"cell_type":"code","execution_count":null,"id":"bade82aa","metadata":{"id":"bade82aa","outputId":"77977155-f984-4407-8a7a-24236904b261","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651429780347,"user_tz":240,"elapsed":133,"user":{"displayName":"Mujahed Darwaza","userId":"06409676498706762728"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["emission_code:\n","A:0 R:1 N:2 D:3 C:4 E:5 Q:6 G:7 H:8 I:9 L:10 K:11 M:12 F:13 P:14 S:15 T:16 W:17 Y:18 V:19 X:20 $:21 \n","\n","state_code:\n","H:0 C:1 E:2 "]}],"source":["emission_code = {residue_list[i]:i for i in range(len(residue_list)-1)}\n","emission_code['$'] = 21\n","state_code = {q3_list[i]:i for i in range(len(q3_list)-1)}\n","\n","print(\"emission_code:\")\n","for k,v in emission_code.items():\n","    print(f\"{k}:{v}\", end=\" \")\n","\n","print(\"\\n\\nstate_code:\")\n","for k,v in state_code.items():\n","    print(f\"{k}:{v}\", end=\" \")"]},{"cell_type":"code","source":["train_df_formatted = format_dataset(train_df, emission_code, state_code, 'q3_expected')\n","dev_df_formatted = format_dataset(dev_df, emission_code, state_code, 'q3_expected')\n","test_df_formatted = format_dataset(test_df, emission_code, state_code, 'q3_expected')   "],"metadata":{"id":"_2s37Bhjd-NR"},"id":"_2s37Bhjd-NR","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"0f86c13d","metadata":{"id":"0f86c13d","outputId":"f17cf643-ff95-4260-df91-131a34e6db50","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651429805503,"user_tz":240,"elapsed":630,"user":{"displayName":"Mujahed Darwaza","userId":"06409676498706762728"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Encoding sequences\n"]}],"source":["print(\"Encoding sequences\")\n","size = 4\n","\n","#make a copy and encode context\n","train_df_context = train_df_formatted.copy()\n","dev_df_context = dev_df_formatted.copy()\n","test_df_context = test_df_formatted.copy()\n","\n","\n","#pad beginning by '$' start symbols determined by lookback/palindrome length\n","#it should be size-1 instead of size if using lookback encoder\n","train_df_context['input'] = train_df_context['input'].apply(lambda x : [21 for i in range(size-1)] + x) \n","dev_df_context['input'] = dev_df_context['input'].apply(lambda x : [21 for i in range(size-1)] + x)\n","test_df_context['input'] = test_df_context['input'].apply(lambda x : [21 for i in range(size-1)] + x)\n","\n","#encodes context and truncates expected vals\n","#replace with desired encoding function\n","\n","train_df_context['input'] = train_df_context['input'].apply(lambda x :  encode_lookback_context(x,size))\n","dev_df_context['input'] = dev_df_context['input'].apply(lambda x :  encode_lookback_context(x,size))\n","test_df_context['input'] = test_df_context['input'].apply(lambda x :  encode_lookback_context(x,size))\n"]},{"cell_type":"markdown","id":"15775fef","metadata":{"id":"15775fef"},"source":["### Step 4: Estimate HMM=(T, E, pi) using Our Data\n"]},{"cell_type":"code","execution_count":null,"id":"f7f85bf8","metadata":{"id":"f7f85bf8","outputId":"5e395ef7-739a-4ddd-86f5-c83f11cdf462","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651429813290,"user_tz":240,"elapsed":4261,"user":{"displayName":"Mujahed Darwaza","userId":"06409676498706762728"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Computing initial estimates for transition and emission matrices using training data\n","Time to estimate T, E, pi is approx: 0.0 minutes\n"]}],"source":["print(\"Computing initial estimates for transition and emission matrices using training data\")\n","start = time.time()\n","T = estimate_transition_matrix(train_df_context, state_code)\n","E = estimate_emission_matrix(train_df_context, state_code, emission_code)\n","pi = start_distribution(train_df_context,state_code)\n","end = time.time()\n","print(f\"Time to estimate T, E, pi is approx: {round((end-start)//60,4)} minutes\")"]},{"cell_type":"markdown","id":"4cd4e5c5","metadata":{"id":"4cd4e5c5"},"source":["### Step 5: Compare HMM against Prior Research [5]\n","\n","    Some slight difference is expected because they were only able to train and test on CB531 whereas we will be training on CB6113 and testing on CB531.\n","\n","    [5] W. Ding, D. Dai, J. Xie, H. Zhang, W. Zhang and H. Xie, \"PRT-HMM: A Novel Hidden Markov Model for Protein Secondary Structure Prediction,\" 2012 IEEE/ACIS 11th International Conference on Computer and Information Science, 2012, pp. 207-212, doi: 10.1109/ICIS.2012.89.\n","\n","    https://ieeexplore-ieee-org.ezproxy.cul.columbia.edu/stamp/stamp.jsp?tp=&arnumber=6211098\n"]},{"cell_type":"code","execution_count":null,"id":"2b13f3b4","metadata":{"id":"2b13f3b4","outputId":"57905032-940a-4595-ada1-7633ee48d806","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651422808224,"user_tz":240,"elapsed":14,"user":{"displayName":"Mujahed Darwaza","userId":"06409676498706762728"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Start Distribution (H,C,E) x (H,C,E):\n","[0.3868 0.3968 0.2164]\n"]}],"source":["print(\"Start Distribution (H,C,E) x (H,C,E):\")\n","print(pi.round(decimals=4))"]},{"cell_type":"code","execution_count":null,"id":"a399c5ab","metadata":{"id":"a399c5ab","outputId":"d0a97755-92b9-4170-fdd6-554df9fee788","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651422808224,"user_tz":240,"elapsed":12,"user":{"displayName":"Mujahed Darwaza","userId":"06409676498706762728"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 0.0372, -0.0437,  0.0064])"]},"metadata":{},"execution_count":13}],"source":["#compare start distribution to source\n","pi_source = np.array([0.3496, 0.4405, 0.2100] )\n","pi_delta = (pi - pi_source).round(decimals=4)\n","pi_delta"]},{"cell_type":"code","execution_count":null,"id":"99c3940f","metadata":{"id":"99c3940f","outputId":"8f14c0fd-2c72-4ace-875d-1e49ac7a4f9e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651422808225,"user_tz":240,"elapsed":10,"user":{"displayName":"Mujahed Darwaza","userId":"06409676498706762728"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Transition Matrix (H,C,E) x (H,C,E):\n","[[0.8989 0.0983 0.0028]\n"," [0.0945 0.8081 0.0974]\n"," [0.0095 0.1721 0.8185]]\n"]}],"source":["print(\"Transition Matrix (H,C,E) x (H,C,E):\")\n","print(T.round(decimals=4))"]},{"cell_type":"code","execution_count":null,"id":"654347a2","metadata":{"id":"654347a2","outputId":"e0a4325f-c9d6-47bc-bea5-305313c2db82","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651422808225,"user_tz":240,"elapsed":8,"user":{"displayName":"Mujahed Darwaza","userId":"06409676498706762728"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 0.0052, -0.0053,  0.0001],\n","       [ 0.0135, -0.0216,  0.0081],\n","       [ 0.0004, -0.008 ,  0.0077]])"]},"metadata":{},"execution_count":15}],"source":["#compare transition matrix to source\n","T_source = np.array( \\\n","    [[0.8937, 0.1036, 0.0027], \\\n","     [0.0810, 0.8297, 0.0893], \\\n","     [0.0091, 0.1801, 0.8108 ]]\n","    )\n","\n","T_delta = (T - T_source).round(decimals=4)\n","T_delta"]},{"cell_type":"code","execution_count":null,"id":"ace595f9","metadata":{"id":"ace595f9","outputId":"2c1f8f3c-afac-453f-8ef3-cd00c6319031","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651422808225,"user_tz":240,"elapsed":7,"user":{"displayName":"Mujahed Darwaza","userId":"06409676498706762728"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Emissions Matrix (H,C,E) x (H,C,E):\n","[[[3.831e-01 6.900e-03 6.270e-02 ... 2.410e-02 5.500e-03 0.000e+00]\n","  [8.460e-02 2.539e-01 6.220e-02 ... 2.260e-02 5.400e-03 2.000e-04]\n","  [7.820e-02 7.200e-03 3.431e-01 ... 2.510e-02 6.000e-03 0.000e+00]\n","  ...\n","  [7.150e-02 8.300e-03 6.630e-02 ... 2.860e-01 5.800e-03 1.000e-04]\n","  [8.630e-02 5.200e-03 8.140e-02 ... 2.970e-02 2.708e-01 3.000e-04]\n","  [9.900e-02 6.200e-03 1.118e-01 ... 2.240e-02 2.240e-02 3.000e-04]]\n","\n"," [[2.833e-01 9.500e-03 3.990e-02 ... 2.070e-02 4.100e-03 0.000e+00]\n","  [4.700e-02 2.577e-01 4.610e-02 ... 2.220e-02 4.900e-03 2.000e-04]\n","  [4.550e-02 9.300e-03 2.873e-01 ... 2.180e-02 3.400e-03 0.000e+00]\n","  ...\n","  [4.520e-02 7.600e-03 4.690e-02 ... 2.320e-01 3.500e-03 1.000e-04]\n","  [4.620e-02 6.900e-03 4.460e-02 ... 2.080e-02 2.910e-01 4.000e-04]\n","  [6.190e-02 1.350e-02 4.920e-02 ... 2.600e-02 2.540e-02 1.000e-04]]\n","\n"," [[2.950e-01 1.170e-02 3.030e-02 ... 3.720e-02 5.700e-03 1.000e-04]\n","  [3.940e-02 3.270e-01 3.450e-02 ... 3.380e-02 5.600e-03 3.000e-04]\n","  [4.670e-02 1.100e-02 2.552e-01 ... 3.920e-02 7.000e-03 1.000e-04]\n","  ...\n","  [4.220e-02 1.330e-02 3.270e-02 ... 3.447e-01 5.400e-03 1.000e-04]\n","  [3.430e-02 8.600e-03 4.790e-02 ... 3.150e-02 2.847e-01 7.000e-04]\n","  [4.160e-02 1.020e-02 4.730e-02 ... 4.960e-02 9.300e-03 3.000e-04]]]\n"]}],"source":["print(\"Emissions Matrix (H,C,E) x (H,C,E):\")\n","print(E.round(decimals=4))"]},{"cell_type":"markdown","id":"c1b19f87","metadata":{"id":"c1b19f87"},"source":["### Step 6: Compute HMM Prediction Performance on Train Data\n","\n","---\n","\n","    We can compare against performance of traditional HMM from [5] as sanity check:\n","        Overall Accuracy: 44.38%\n","        Helix Accuracy (H): 90.46%\n","        Beta-Sheet Accuracy (E): 4.56%\n","        Coil Accuracy (C): 28.05%\n","        \n","     *Some slight difference is expected because they were only able to train and test on CB531 whereas we will be training on CB6113 and testing on CB531.\n","    "]},{"cell_type":"code","execution_count":null,"id":"15c97cdc","metadata":{"id":"15c97cdc","outputId":"1348bfe3-f164-4dbe-d886-61b61975b757","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651422855473,"user_tz":240,"elapsed":47253,"user":{"displayName":"Mujahed Darwaza","userId":"06409676498706762728"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Time predict on training data is approx: 47.25 seconds\n"]}],"source":["#make predictions\n","start = time.time()\n","train_predictions = getPredictions(train_df_context, T, E, pi)\n","train_acc, train_prec, train_rec, train_cnts = HMMaccuracy(train_predictions, q=3)\n","end = time.time()\n","print(f\"Time predict on training data is approx: {round((end-start),2)} seconds\")"]},{"cell_type":"code","execution_count":null,"id":"d544fadd","metadata":{"id":"d544fadd","outputId":"eba120cf-b3c6-4ee7-81a8-c43dc6a88038","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651422855473,"user_tz":240,"elapsed":21,"user":{"displayName":"Mujahed Darwaza","userId":"06409676498706762728"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.4157 \n","\n","Precision (H,C,E):\n","\t [0.4028 0.6559 0.4887] \n","\n","Recall (H,C,E):\n","\t [0.9834 0.0811 0.0147] \n","\n","Counts (H,C,E) x (H,C,E):\n","[[394015   5401   1257]\n"," [375447  33331   2190]\n"," [208803  12083   3295]]\n"]}],"source":["#Our HMM performance\n","\n","print(f\"Accuracy: {round(train_acc,4)} \\n\")\n","print(f\"Precision (H,C,E):\\n\\t {train_prec.round(decimals = 4)} \\n\")\n","print(f\"Recall (H,C,E):\\n\\t {train_rec.round(decimals = 4)} \\n\")\n","print(\"Counts (H,C,E) x (H,C,E):\")\n","print(train_cnts)"]},{"cell_type":"markdown","id":"334414bf","metadata":{"id":"334414bf"},"source":["### Step 7: Compute HMM Performance on Dev Data\n","\n","    We can compare against performance of traditional HMM from [5] as sanity check:\n","            Overall Accuracy: 44.38%\n","            Helix Accuracy (H): 90.46%\n","            Beta-Sheet Accuracy (E): 4.56%\n","            Coil Accuracy (C): 28.05%\n","        \n","     *Some slight difference is expected because they were only able to train and test on CB531 whereas we will be training on CB6113 and testing on CB531."]},{"cell_type":"code","execution_count":null,"id":"09ec68c6","metadata":{"id":"09ec68c6","outputId":"624a5b69-74b3-479f-a47c-2419fef7732d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651422860474,"user_tz":240,"elapsed":5019,"user":{"displayName":"Mujahed Darwaza","userId":"06409676498706762728"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Time predict on dev data is approx: 4.98 seconds\n"]}],"source":["start = time.time()\n","dev_predictions = getPredictions(dev_df_context, T, E, pi)\n","dev_acc, dev_prec, dev_rec, dev_cnts = HMMaccuracy(dev_predictions, q=3)\n","end = time.time()\n","print(f\"Time predict on dev data is approx: {round((end-start),2)} seconds\")"]},{"cell_type":"code","execution_count":null,"id":"a639b81d","metadata":{"id":"a639b81d","outputId":"eff04725-14fe-49dd-f4db-4f229309811f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651422860475,"user_tz":240,"elapsed":15,"user":{"displayName":"Mujahed Darwaza","userId":"06409676498706762728"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.4106 \n","\n","Precision (H,C,E):\n","\t [0.395  0.6449 0.6048] \n","\n","Recall (H,C,E):\n","\t [0.9844 0.0926 0.0162] \n","\n","Counts (H,C,E) x (H,C,E):\n","[[43878   648    47]\n"," [42771  4389   234]\n"," [24424  1769   430]]\n"]}],"source":["print(f\"Accuracy: {round(dev_acc,4)} \\n\")\n","print(f\"Precision (H,C,E):\\n\\t {dev_prec.round(decimals = 4)} \\n\")\n","print(f\"Recall (H,C,E):\\n\\t {dev_rec.round(decimals = 4)} \\n\")\n","print(\"Counts (H,C,E) x (H,C,E):\")\n","print(dev_cnts)"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.11"},"colab":{"name":"Pre-processing emissions.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":5}