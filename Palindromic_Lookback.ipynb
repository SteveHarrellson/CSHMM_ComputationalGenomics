{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f248ddf3",
      "metadata": {
        "id": "f248ddf3"
      },
      "source": [
        "# Prediciting Q3 protein secondary structure using HMM with pre-processed emission sequence\n",
        "\n",
        "Two functions have been coded to pre-process the emission sequence. Using this code, one can either use the lookback encoder to mimic an alpha helix interaction of the amino acids, or the palindromic encoder for a beta sheet modelling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aca0128a",
      "metadata": {
        "id": "aca0128a"
      },
      "outputs": [],
      "source": [
        "import gzip\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abc50fe5",
      "metadata": {
        "id": "abc50fe5"
      },
      "outputs": [],
      "source": [
        "def get_data(arr, residue_list, q8_list, columns, r, f, bounds=None):\n",
        "    \n",
        "    \"\"\"\n",
        "    This function retrieves and formats data from the CB6133_filtered and CB531 datasets [1][2]\n",
        "    Codes is slighlty modified from code provided by [3][4]\n",
        "    \n",
        "    [1] Jian Zhou and Olga G. Troyanskaya. Deep supervised and convolutional generative stochastic network for\n",
        "        protein s\n",
        "    [2] Jian Zhou and Olga G. Troyanskaya. CB6133 dataset.\n",
        "        https://www.princeton.edu/~jzthree/datasets/ICML2014/dataset_readme.txt, 2014.\n",
        "    [3] Iddo Drori et al. High Quality Prediction of Protein Q8 Secondary Structure by\n",
        "        Diverse Neural Network Architectures. arXiv preprint arXiv:1811.07143, 2018\n",
        "    [4] https://github.com/idrori/cu-ssp/blob/master/model_1/model_1.py\n",
        "    \"\"\"\n",
        "    \n",
        "    if bounds is None: bounds = range(len(arr))\n",
        "    \n",
        "    data = [None for i in bounds]\n",
        "    for i in bounds:\n",
        "        seq, q8, q3, q2, profiles = '', '', '', '', []\n",
        "        for j in range(r):\n",
        "            jf = j*f\n",
        "            \n",
        "            # Residue convert from one-hot to decoded\n",
        "            residue_onehot = arr[i,jf+0:jf+22]\n",
        "            residue = residue_list[np.argmax(residue_onehot)]\n",
        "\n",
        "            # Q8 one-hot encoded to decoded structure symbol\n",
        "            residue_q8_onehot = arr[i,jf+22:jf+31]\n",
        "            residue_q8 = q8_list[np.argmax(residue_q8_onehot)]\n",
        "\n",
        "            if residue == 'NoSeq': break      # terminating sequence symbol\n",
        "\n",
        "            nc_terminals = arr[i,jf+31:jf+33] # nc_terminals = [0. 0.]\n",
        "            sa = arr[i,jf+33:jf+35]           # sa = [0. 0.]\n",
        "            profile = arr[i,jf+35:jf+57]      # profile features\n",
        "            \n",
        "            seq += residue # concat residues into amino acid sequence\n",
        "\n",
        "            #encode q3 structure\n",
        "            if residue_q8 in 'GHI':\n",
        "                q3 += 'H'\n",
        "                q2 += 'A'\n",
        "            elif residue_q8 in 'TBSL':\n",
        "                q3 += 'C'\n",
        "                q2 += 'X'\n",
        "            elif residue_q8 in 'E':\n",
        "                q3 += 'E'\n",
        "                q2 += 'X'\n",
        "            else:\n",
        "                q3 += 'Z'\n",
        "                q2 += 'Z'\n",
        "            \n",
        "            q8  += residue_q8 # concat secondary structure into secondary structure sequence\n",
        "            profiles.append(profile)\n",
        "        \n",
        "        data[i] = [str(i+1), len(seq), seq, np.array(profiles), q8, q3, q2]\n",
        "    \n",
        "    return pd.DataFrame(data, columns=columns)\n",
        "\n",
        "\n",
        "def encode_sequence(sequence, code):\n",
        "    \n",
        "    \"\"\"\n",
        "    Provided an input sequence and a code, returns the encoding of the sequence\n",
        "    \"\"\"\n",
        "    \n",
        "    encoded_seq = []\n",
        "    \n",
        "    for x in sequence:\n",
        "        try:\n",
        "            idx = code[x]\n",
        "            encoded_seq.append(idx)\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "            break\n",
        "    \n",
        "    return encoded_seq\n",
        "\n",
        "\n",
        "def format_dataset(df, emission_code, state_code, exp_col=\"q3_expected\"):\n",
        "    \n",
        "    \"\"\"\n",
        "    Provided a dataframe which contains the amino sequences and the hidden sequence,\n",
        "    this function encodes those sequences according to the provided codes\n",
        "    and return them\n",
        "    \n",
        "    *exp_col specifies if want to encode the q8, q3, or q2 hidden sequence\n",
        "    \"\"\"\n",
        "    \n",
        "    assert ('id' in df.columns and 'len' in df.columns and 'input' in df.columns and exp_col in df.columns)\n",
        "    \n",
        "    formattedDF = pd.DataFrame(columns=['id','len','input','expected'])\n",
        "\n",
        "    for i in range(len(df)):\n",
        "        \n",
        "        sid = df.iloc[i].id\n",
        "        slen = df.iloc[i].len\n",
        "        enc_input = encode_sequence(df.iloc[i].input, emission_code)\n",
        "        enc_expected = encode_sequence(df.iloc[i][exp_col], state_code)\n",
        "        \n",
        "        assert (len(enc_input) == len(enc_expected))\n",
        "        \n",
        "        formattedDF = formattedDF.append({'id':sid, 'len':slen, 'input':enc_input, 'expected':enc_expected}, ignore_index=True)\n",
        "\n",
        "    return formattedDF\n",
        "\n",
        "def estimate_transition_matrix(df, state_code):\n",
        "    \"\"\"\n",
        "    Given a dataframe that has the data for the amino sequences and their corresponding hidden sequence,\n",
        "    we use the data to compute the MLEs of the emission probablities.\n",
        "    \n",
        "    ex. estimated P(emission=A|state=H) = count(emission=A,state=H) / sum_over_all_emission count(emission, state=H)\n",
        "    \n",
        "    *implemented a pseudocount of +1 for cases where we have 0 observations of a certain (emission,state) combo\n",
        "    \"\"\"\n",
        "    \n",
        "    n_states = len(state_code)\n",
        "    \n",
        "    #using pseudocount of +1\n",
        "    counts = np.ones(shape=(n_states, n_states), dtype=float)\n",
        "    \n",
        "    for i in range(len(df)):\n",
        "        \n",
        "        state_seq = df.iloc[i].expected\n",
        "        seq_len = len(df.iloc[i].expected)\n",
        "        \n",
        "        for j in range(seq_len - 1):\n",
        "            \n",
        "            x = state_seq[j]\n",
        "            y = state_seq[j+1]\n",
        "            \n",
        "            counts[x,y] += 1\n",
        "    \n",
        "    #transform counts to probability by normalizing of row sums\n",
        "    row_sums = np.sum(counts, axis=1)\n",
        "    T = counts / row_sums.reshape((-1,1))\n",
        "    \n",
        "    return T\n",
        "\n",
        "\n",
        "def estimate_emission_matrix(df, state_code, emission_code):\n",
        "    \"\"\"\n",
        "    Given a dataframe that has the data for the amino sequences and their corresponding hidden sequence,\n",
        "    we use the data to compute the MLEs of the transition probablities.\n",
        "    \n",
        "    ex. estimated P(state_{t+1}=E|state_{t}=H) = count(state_{t}=H, state_{t+1}=E) / sum_over_all_states count(state_{t}=H, state_{t+1})\n",
        "    \n",
        "    *implemented a pseudocount of +1 for cases where we have 0 observations of a certain (state,state) combo\n",
        "    \"\"\"\n",
        "    \n",
        "    n_states = len(state_code)\n",
        "    n_emissions = len(emission_code)\n",
        "    \n",
        "    #using pseudocount of +1\n",
        "    #Steve Contribution\n",
        "    #Change counts matrix to be n_states x n_emissions x n_emissions\n",
        "    #Each state has a n_emissions x n_emissions context-dependent matrix associated with it\n",
        "    counts = np.ones(shape=(n_states, n_emissions, n_emissions), dtype=float)\n",
        "    \n",
        "    for i in range(len(df)):\n",
        "        \n",
        "        state_seq = df.iloc[i].expected\n",
        "        #emission_seq now of form: [(1,4),(2,5),...]\n",
        "        #store context in y\n",
        "        #store new state in z\n",
        "        emission_seq = df.iloc[i].input\n",
        "        #df['len'] no longer reflects true length\n",
        "        seq_len = len(df.iloc[i].input)\n",
        "        \n",
        "        for j in range(seq_len):\n",
        "            \n",
        "            x = state_seq[j]\n",
        "            y = emission_seq[j][0]\n",
        "            z = emission_seq[j][1]\n",
        "            \n",
        "            counts[x,y,z] += 1\n",
        "\n",
        "    #transform counts to probability by normalizing of row sums\n",
        "    #not sure how pendo's way works, implementing my own here\n",
        "    #index y is 'context', normalize by this row\n",
        "    for i in range(counts.shape[0]):\n",
        "        for j in range(counts.shape[1]):\n",
        "            counts[i,j,:] = counts[i,j,:]/np.sum(counts[i,j,:])\n",
        "    \n",
        "    E = counts\n",
        "    \n",
        "    #row_sums = np.sum(counts, axis=1)\n",
        "    #print(row_sums.shape)\n",
        "    #E = counts / row_sums.reshape((-1,3))\n",
        "\n",
        "    return E\n",
        "\n",
        "def start_distribution(df,state_code):\n",
        "    \"\"\"\n",
        "    Given a dataframe that has the data for the amino sequences and their corresponding hidden sequence,\n",
        "    we use the data to compute the MLEs of the start distribution.\n",
        "    \n",
        "    ex. estimated P(state=H) = count(state=H) / sum_over_all_states count(state)\n",
        "    \n",
        "    *implemented a pseudocount of +1 for cases where we have 0 observations of a certain state\n",
        "    \"\"\"\n",
        "    \n",
        "    n_states = len(state_code)\n",
        "    \n",
        "    #using pseudocount of +1\n",
        "    counts = np.array([1.0] * n_states)\n",
        "    \n",
        "    for i in range(len(df)):\n",
        "        \n",
        "        state_seq = df.iloc[i].expected\n",
        "        seq_len = len(df.iloc[i].expected)\n",
        "        \n",
        "        for j in range(seq_len):\n",
        "            \n",
        "            x = state_seq[j]\n",
        "            \n",
        "            counts[x] += 1\n",
        "    \n",
        "    #transform counts to probability by normalizing of row sums\n",
        "    total = sum(counts)\n",
        "    pi = counts / total\n",
        "    \n",
        "    return pi\n",
        "\n",
        "def viterbi_decoding(T,E,pi,seq):\n",
        "    \"\"\"\n",
        "    This functions performs viterbi decoding to get the predicted hidden sequence,\n",
        "    given the input emission sequence as well as the transition matrix, emission matrix,\n",
        "    and the start distribution\n",
        "    \"\"\"\n",
        "    \n",
        "    #sequence length\n",
        "    N = len(seq)\n",
        "    #num of states\n",
        "    M = T.shape[0]\n",
        "    \n",
        "    assert (M == len(pi))\n",
        "    \n",
        "    #V will store viterbi values\n",
        "    V = np.zeros(shape=(M, N), dtype=float)\n",
        "    \n",
        "    #P will store prev state from which we transitioned into state m and time n to achieve the max value of V[m,n]\n",
        "    #(i.e. pointer to help us reconstruct sequence after predicting most probable path in viterbi graph)\n",
        "    P = np.empty(shape=(M, N))\n",
        "    P[:] = np.NaN\n",
        "    \n",
        "    #populate viterbi matrix\n",
        "    for n in range(N):\n",
        "        \n",
        "        #get current emissions\n",
        "        e = seq[n]\n",
        "        \n",
        "        for m in range(M):\n",
        "            \n",
        "            #initilize viterbi value for current timestep given state m to be -infty\n",
        "            maxV = float(\"-inf\")\n",
        "            prev = np.NaN\n",
        "            \n",
        "            #get log prob. of emission given state m\n",
        "            emiss_logp = np.log(E[m,e[0],e[1]])\n",
        "            \n",
        "            #start of sequence\n",
        "            if n == 0:\n",
        "                start_logp = np.log(pi[m])\n",
        "                maxV = emiss_logp + start_logp\n",
        "                \n",
        "            else:\n",
        "            \n",
        "                #solve for max value for V[m,n]\n",
        "                for i in range(M): \n",
        "\n",
        "                    #get previous timestep viterbi value for state i (which should be a log prob)\n",
        "                    prev_vit = V[i, n-1]\n",
        "\n",
        "                    #get log prob of transition from state i to m\n",
        "                    trans_logp = np.log(T[i,m])\n",
        "\n",
        "                    #update viterbi value for current timestep given state m\n",
        "                    curV = prev_vit + trans_logp + emiss_logp\n",
        "\n",
        "                    if curV > maxV:\n",
        "                        maxV = curV\n",
        "                        prev = i\n",
        "        \n",
        "            V[m,n] = maxV\n",
        "            P[m,n] = prev\n",
        "    \n",
        "    #initialize with state with highest probability at end of sequence\n",
        "    best_path = [np.argmax(V[:,-1])]\n",
        "    \n",
        "    #work backwards to reconstruct sequence\n",
        "    for n in range(N-1,0,-1):\n",
        "        \n",
        "        #determine where we are\n",
        "        cur_state = best_path[0]\n",
        "\n",
        "        #find state from which we came that yielded highest probability to current state at current time\n",
        "        prev_state = int(P[cur_state,n])\n",
        "\n",
        "        #prepend previous state\n",
        "        best_path = [prev_state] + best_path\n",
        "    \n",
        "    return best_path\n",
        "\n",
        "\n",
        "def getPredictions(df, T, E, pi):\n",
        "    \n",
        "    \"\"\"\n",
        "    get predictions for all sequences in specified dataset using provided HMM\n",
        "    \"\"\"\n",
        "    \n",
        "    results = pd.DataFrame(columns=['input','predicted','expected'])\n",
        "    \n",
        "    for i in range(len(df)):\n",
        "        \n",
        "        #change amino_seq input\n",
        "        amino_seq = df.iloc[i].input\n",
        "        pred_seq = viterbi_decoding(T,E,pi,amino_seq)\n",
        "        exp_seq = df.iloc[i].expected\n",
        "        \n",
        "        #Steve commented this out because we are padding our start with new symbols\n",
        "        #assert(len(amino_seq) == len(pred_seq) and len(amino_seq) == len(exp_seq))\n",
        "        \n",
        "        results = results.append({'input':amino_seq, 'predicted':pred_seq, 'expected':exp_seq}, ignore_index=True)\n",
        "    \n",
        "    return results\n",
        "\n",
        "def HMMaccuracy(df,q=3):\n",
        "    \n",
        "    \"\"\"\n",
        "    Compute accurcay of HMM given a dataframe the has the input emission sequences,\n",
        "    the predicted hidden sequences, and the actual hidden sequences\n",
        "    \n",
        "    *q specifies whether the predicion was made for q2, q3, or q8 protein structure\n",
        "    \"\"\"\n",
        "    \n",
        "    #row represents expected state\n",
        "    #col represents predicted state\n",
        "    counts = np.zeros(shape=(q,q), dtype=int)\n",
        "    \n",
        "    for i in range(len(df)):\n",
        "\n",
        "        #get predicted and expected hidden sequence from dataframe\n",
        "        pred = df.iloc[i].predicted\n",
        "        exp = df.iloc[i].expected\n",
        "        \n",
        "        #assert (len(pred) == len(exp))\n",
        "        \n",
        "        for j in range(len(pred)):\n",
        "            \n",
        "            x = exp[j]\n",
        "            y = pred[j]\n",
        "            counts[x,y] += 1\n",
        "    \n",
        "    rowSum = np.sum(counts, axis=1)\n",
        "    colSum = np.sum(counts, axis=0)\n",
        "    \n",
        "    #true positive (negative) / total predicted positive (negative)\n",
        "    precision = np.array([counts[i,i] / colSum[i] for i in range(q)])\n",
        "    \n",
        "    #true positive (negative) / total actual positive (negative)\n",
        "    recall = np.array([counts[i,i] / rowSum[i] for i in range(q)])\n",
        "    \n",
        "    accuracy = 0\n",
        "    for i in range(q):\n",
        "        accuracy += counts[i,i]\n",
        "    accuracy = accuracy / sum(rowSum)\n",
        "    \n",
        "    \n",
        "    return accuracy, precision, recall, counts\n",
        "\n",
        "#Pre-processing functions for emissions\n",
        "\n",
        "def encode_lookback_context(sequence,spacing = 4):\n",
        "  '''Encode context by keeping track of the i-spacing residue'''\n",
        "  encoded_sequence = []\n",
        "  for i in range(spacing,len(sequence)):\n",
        "      encoded_sequence.append((sequence[i-spacing],sequence[i]))\n",
        "  return encoded_sequence\n",
        "\n",
        "def encode_palindrome_context(sequence, length):\n",
        "  '''\n",
        "  Encode context by associating amino acids around a turn together. This function supposes that each strand is of fixed length, and then turns.\n",
        "  '''\n",
        "  encoded_sequence = []\n",
        "  for i in range(length,len(sequence)):\n",
        "    encoded_sequence.append((sequence[i-2*(i%length)],sequence[i]))\n",
        "  return encoded_sequence\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l = range(1,20)\n",
        "#Example of lookback encoding\n",
        "print(encode_lookback_context(l))\n",
        "\n",
        "#Example of palindrome encoding. \n",
        "#There is a turn every 5 amino acids. The second number of the tuple is always increasing at a constant rate.\n",
        "print(encode_palindrome_context(l,5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFJdc4ZtGvnt",
        "outputId": "d02dca6d-f973-4e64-f618-7057bde5d694"
      },
      "id": "dFJdc4ZtGvnt",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(1, 5), (2, 6), (3, 7), (4, 8), (5, 9), (6, 10), (7, 11), (8, 12), (9, 13), (10, 14), (11, 15), (12, 16), (13, 17), (14, 18), (15, 19)]\n",
            "[(6, 6), (5, 7), (4, 8), (3, 9), (2, 10), (11, 11), (10, 12), (9, 13), (8, 14), (7, 15), (16, 16), (15, 17), (14, 18), (13, 19)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "984ee1c9",
      "metadata": {
        "id": "984ee1c9"
      },
      "source": [
        "Step 1: Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a55243b5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a55243b5",
        "outputId": "b6c09a2e-891a-4459-8f22-e28842533e50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Loaded\n",
            "CB6133 shape: (5365, 39900)\n",
            "CB513 shape: (514, 39900)\n"
          ]
        }
      ],
      "source": [
        "#seed so get consistent results for every run\n",
        "random.seed(0)\n",
        "\n",
        "cb513 = np.load('cb513+profile_split1.npy.gz')\n",
        "cb6133filtered = np.load('cullpdb+profile_5926_filtered.npy.gz')\n",
        "print(\"Data Loaded\")\n",
        "print(f\"CB6133 shape: {cb6133filtered.shape}\")\n",
        "print(f\"CB513 shape: {cb513.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c910596f",
      "metadata": {
        "id": "c910596f"
      },
      "source": [
        "### Step 2: Process Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "091dae2b",
      "metadata": {
        "id": "091dae2b",
        "outputId": "9f71003e-251e-40df-8b06-819669f7df82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Turning data arrays into dataframes\n"
          ]
        }
      ],
      "source": [
        "maxlen_seq = r = 700 # protein residues padded to 700\n",
        "f = 57  # number of features for each residue\n",
        "\n",
        "residue_list = list('ARNDCEQGHILKMFPSTWYVX') + ['NoSeq']\n",
        "q8_list      = list('LBEGIHST') + ['NoSeq']\n",
        "q3_list      = list('HCE') + ['NoSeq']\n",
        "q2_list      = list('AX') + ['NoSeq']\n",
        "\n",
        "columns = [\"id\", \"len\", \"input\", \"profiles\", \"q8_expected\", \"q3_expected\", \"q2_expected\"]\n",
        "\n",
        "print(\"Turning data arrays into dataframes\")\n",
        "\n",
        "# train, dev, test split\n",
        "# break out 10% of train data to be used as dev set\n",
        "train_df, dev_df = train_test_split(get_data(cb6133filtered, residue_list, q8_list, columns, r, f), test_size=0.1)\n",
        "test_df  = get_data(cb513, residue_list, q8_list, columns, r, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73751055",
      "metadata": {
        "id": "73751055"
      },
      "source": [
        "### Step 3: Encode Sequences and Format DataFrames\n",
        "    (a) Create codes to encode emission and hidden sequences\n",
        "    (b) Apply encodings & specify hidden sequence of interest (q2, q3, q8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bade82aa",
      "metadata": {
        "id": "bade82aa",
        "outputId": "77977155-f984-4407-8a7a-24236904b261",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "emission_code:\n",
            "A:0 R:1 N:2 D:3 C:4 E:5 Q:6 G:7 H:8 I:9 L:10 K:11 M:12 F:13 P:14 S:15 T:16 W:17 Y:18 V:19 X:20 $:21 \n",
            "\n",
            "state_code:\n",
            "H:0 C:1 E:2 "
          ]
        }
      ],
      "source": [
        "emission_code = {residue_list[i]:i for i in range(len(residue_list)-1)}\n",
        "emission_code['$'] = 21\n",
        "state_code = {q3_list[i]:i for i in range(len(q3_list)-1)}\n",
        "\n",
        "print(\"emission_code:\")\n",
        "for k,v in emission_code.items():\n",
        "    print(f\"{k}:{v}\", end=\" \")\n",
        "\n",
        "print(\"\\n\\nstate_code:\")\n",
        "for k,v in state_code.items():\n",
        "    print(f\"{k}:{v}\", end=\" \")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df_formatted = format_dataset(train_df, emission_code, state_code, 'q3_expected')\n",
        "dev_df_formatted = format_dataset(dev_df, emission_code, state_code, 'q3_expected')\n",
        "test_df_formatted = format_dataset(test_df, emission_code, state_code, 'q3_expected')   "
      ],
      "metadata": {
        "id": "_2s37Bhjd-NR"
      },
      "id": "_2s37Bhjd-NR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f86c13d",
      "metadata": {
        "id": "0f86c13d",
        "outputId": "f17cf643-ff95-4260-df91-131a34e6db50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding sequences\n"
          ]
        }
      ],
      "source": [
        "print(\"Encoding sequences\")\n",
        "size = 4\n",
        "\n",
        "#make a copy and encode context\n",
        "train_df_context = train_df_formatted.copy()\n",
        "dev_df_context = dev_df_formatted.copy()\n",
        "test_df_context = test_df_formatted.copy()\n",
        "\n",
        "\n",
        "#pad beginning by '$' start symbols determined by lookback/palindrome length\n",
        "#it should be size-1 instead of size if using lookback encoder\n",
        "train_df_context['input'] = train_df_context['input'].apply(lambda x : [21 for i in range(size-1)] + x) \n",
        "dev_df_context['input'] = dev_df_context['input'].apply(lambda x : [21 for i in range(size-1)] + x)\n",
        "test_df_context['input'] = test_df_context['input'].apply(lambda x : [21 for i in range(size-1)] + x)\n",
        "\n",
        "#encodes context and truncates expected vals\n",
        "#replace with desired encoding function\n",
        "\n",
        "train_df_context['input'] = train_df_context['input'].apply(lambda x :  encode_lookback_context(x,size))\n",
        "dev_df_context['input'] = dev_df_context['input'].apply(lambda x :  encode_lookback_context(x,size))\n",
        "test_df_context['input'] = test_df_context['input'].apply(lambda x :  encode_lookback_context(x,size))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15775fef",
      "metadata": {
        "id": "15775fef"
      },
      "source": [
        "### Step 4: Estimate HMM=(T, E, pi) using Our Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7f85bf8",
      "metadata": {
        "id": "f7f85bf8",
        "outputId": "5e395ef7-739a-4ddd-86f5-c83f11cdf462",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing initial estimates for transition and emission matrices using training data\n",
            "Time to estimate T, E, pi is approx: 0.0 minutes\n"
          ]
        }
      ],
      "source": [
        "print(\"Computing initial estimates for transition and emission matrices using training data\")\n",
        "start = time.time()\n",
        "T = estimate_transition_matrix(train_df_context, state_code)\n",
        "E = estimate_emission_matrix(train_df_context, state_code, emission_code)\n",
        "pi = start_distribution(train_df_context,state_code)\n",
        "end = time.time()\n",
        "print(f\"Time to estimate T, E, pi is approx: {round((end-start)//60,4)} minutes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4cd4e5c5",
      "metadata": {
        "id": "4cd4e5c5"
      },
      "source": [
        "### Step 5: Compare HMM against Prior Research [5]\n",
        "\n",
        "    Some slight difference is expected because they were only able to train and test on CB531 whereas we will be training on CB6113 and testing on CB531.\n",
        "\n",
        "    [5] W. Ding, D. Dai, J. Xie, H. Zhang, W. Zhang and H. Xie, \"PRT-HMM: A Novel Hidden Markov Model for Protein Secondary Structure Prediction,\" 2012 IEEE/ACIS 11th International Conference on Computer and Information Science, 2012, pp. 207-212, doi: 10.1109/ICIS.2012.89.\n",
        "\n",
        "    https://ieeexplore-ieee-org.ezproxy.cul.columbia.edu/stamp/stamp.jsp?tp=&arnumber=6211098\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b13f3b4",
      "metadata": {
        "id": "2b13f3b4",
        "outputId": "57905032-940a-4595-ada1-7633ee48d806",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Distribution (H,C,E) x (H,C,E):\n",
            "[0.3868 0.3968 0.2164]\n"
          ]
        }
      ],
      "source": [
        "print(\"Start Distribution (H,C,E) x (H,C,E):\")\n",
        "print(pi.round(decimals=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a399c5ab",
      "metadata": {
        "id": "a399c5ab",
        "outputId": "d0a97755-92b9-4170-fdd6-554df9fee788",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.0372, -0.0437,  0.0064])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "#compare start distribution to source\n",
        "pi_source = np.array([0.3496, 0.4405, 0.2100] )\n",
        "pi_delta = (pi - pi_source).round(decimals=4)\n",
        "pi_delta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99c3940f",
      "metadata": {
        "id": "99c3940f",
        "outputId": "8f14c0fd-2c72-4ace-875d-1e49ac7a4f9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transition Matrix (H,C,E) x (H,C,E):\n",
            "[[0.8989 0.0983 0.0028]\n",
            " [0.0945 0.8081 0.0974]\n",
            " [0.0095 0.1721 0.8185]]\n"
          ]
        }
      ],
      "source": [
        "print(\"Transition Matrix (H,C,E) x (H,C,E):\")\n",
        "print(T.round(decimals=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "654347a2",
      "metadata": {
        "id": "654347a2",
        "outputId": "e0a4325f-c9d6-47bc-bea5-305313c2db82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.0052, -0.0053,  0.0001],\n",
              "       [ 0.0135, -0.0216,  0.0081],\n",
              "       [ 0.0004, -0.008 ,  0.0077]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "#compare transition matrix to source\n",
        "T_source = np.array( \\\n",
        "    [[0.8937, 0.1036, 0.0027], \\\n",
        "     [0.0810, 0.8297, 0.0893], \\\n",
        "     [0.0091, 0.1801, 0.8108 ]]\n",
        "    )\n",
        "\n",
        "T_delta = (T - T_source).round(decimals=4)\n",
        "T_delta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ace595f9",
      "metadata": {
        "id": "ace595f9",
        "outputId": "2c1f8f3c-afac-453f-8ef3-cd00c6319031",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Emissions Matrix (H,C,E) x (H,C,E):\n",
            "[[[3.831e-01 6.900e-03 6.270e-02 ... 2.410e-02 5.500e-03 0.000e+00]\n",
            "  [8.460e-02 2.539e-01 6.220e-02 ... 2.260e-02 5.400e-03 2.000e-04]\n",
            "  [7.820e-02 7.200e-03 3.431e-01 ... 2.510e-02 6.000e-03 0.000e+00]\n",
            "  ...\n",
            "  [7.150e-02 8.300e-03 6.630e-02 ... 2.860e-01 5.800e-03 1.000e-04]\n",
            "  [8.630e-02 5.200e-03 8.140e-02 ... 2.970e-02 2.708e-01 3.000e-04]\n",
            "  [9.900e-02 6.200e-03 1.118e-01 ... 2.240e-02 2.240e-02 3.000e-04]]\n",
            "\n",
            " [[2.833e-01 9.500e-03 3.990e-02 ... 2.070e-02 4.100e-03 0.000e+00]\n",
            "  [4.700e-02 2.577e-01 4.610e-02 ... 2.220e-02 4.900e-03 2.000e-04]\n",
            "  [4.550e-02 9.300e-03 2.873e-01 ... 2.180e-02 3.400e-03 0.000e+00]\n",
            "  ...\n",
            "  [4.520e-02 7.600e-03 4.690e-02 ... 2.320e-01 3.500e-03 1.000e-04]\n",
            "  [4.620e-02 6.900e-03 4.460e-02 ... 2.080e-02 2.910e-01 4.000e-04]\n",
            "  [6.190e-02 1.350e-02 4.920e-02 ... 2.600e-02 2.540e-02 1.000e-04]]\n",
            "\n",
            " [[2.950e-01 1.170e-02 3.030e-02 ... 3.720e-02 5.700e-03 1.000e-04]\n",
            "  [3.940e-02 3.270e-01 3.450e-02 ... 3.380e-02 5.600e-03 3.000e-04]\n",
            "  [4.670e-02 1.100e-02 2.552e-01 ... 3.920e-02 7.000e-03 1.000e-04]\n",
            "  ...\n",
            "  [4.220e-02 1.330e-02 3.270e-02 ... 3.447e-01 5.400e-03 1.000e-04]\n",
            "  [3.430e-02 8.600e-03 4.790e-02 ... 3.150e-02 2.847e-01 7.000e-04]\n",
            "  [4.160e-02 1.020e-02 4.730e-02 ... 4.960e-02 9.300e-03 3.000e-04]]]\n"
          ]
        }
      ],
      "source": [
        "print(\"Emissions Matrix (H,C,E) x (H,C,E):\")\n",
        "print(E.round(decimals=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1b19f87",
      "metadata": {
        "id": "c1b19f87"
      },
      "source": [
        "### Step 6: Compute HMM Prediction Performance on Train Data\n",
        "\n",
        "---\n",
        "\n",
        "    We can compare against performance of traditional HMM from [5] as sanity check:\n",
        "        Overall Accuracy: 44.38%\n",
        "        Helix Accuracy (H): 90.46%\n",
        "        Beta-Sheet Accuracy (E): 4.56%\n",
        "        Coil Accuracy (C): 28.05%\n",
        "        \n",
        "     *Some slight difference is expected because they were only able to train and test on CB531 whereas we will be training on CB6113 and testing on CB531.\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15c97cdc",
      "metadata": {
        "id": "15c97cdc",
        "outputId": "1348bfe3-f164-4dbe-d886-61b61975b757",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time predict on training data is approx: 47.25 seconds\n"
          ]
        }
      ],
      "source": [
        "#make predictions\n",
        "start = time.time()\n",
        "train_predictions = getPredictions(train_df_context, T, E, pi)\n",
        "train_acc, train_prec, train_rec, train_cnts = HMMaccuracy(train_predictions, q=3)\n",
        "end = time.time()\n",
        "print(f\"Time predict on training data is approx: {round((end-start),2)} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d544fadd",
      "metadata": {
        "id": "d544fadd",
        "outputId": "eba120cf-b3c6-4ee7-81a8-c43dc6a88038",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.4157 \n",
            "\n",
            "Precision (H,C,E):\n",
            "\t [0.4028 0.6559 0.4887] \n",
            "\n",
            "Recall (H,C,E):\n",
            "\t [0.9834 0.0811 0.0147] \n",
            "\n",
            "Counts (H,C,E) x (H,C,E):\n",
            "[[394015   5401   1257]\n",
            " [375447  33331   2190]\n",
            " [208803  12083   3295]]\n"
          ]
        }
      ],
      "source": [
        "#Our HMM performance\n",
        "\n",
        "print(f\"Accuracy: {round(train_acc,4)} \\n\")\n",
        "print(f\"Precision (H,C,E):\\n\\t {train_prec.round(decimals = 4)} \\n\")\n",
        "print(f\"Recall (H,C,E):\\n\\t {train_rec.round(decimals = 4)} \\n\")\n",
        "print(\"Counts (H,C,E) x (H,C,E):\")\n",
        "print(train_cnts)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "334414bf",
      "metadata": {
        "id": "334414bf"
      },
      "source": [
        "### Step 7: Compute HMM Performance on Dev Data\n",
        "\n",
        "    We can compare against performance of traditional HMM from [5] as sanity check:\n",
        "            Overall Accuracy: 44.38%\n",
        "            Helix Accuracy (H): 90.46%\n",
        "            Beta-Sheet Accuracy (E): 4.56%\n",
        "            Coil Accuracy (C): 28.05%\n",
        "        \n",
        "     *Some slight difference is expected because they were only able to train and test on CB531 whereas we will be training on CB6113 and testing on CB531."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09ec68c6",
      "metadata": {
        "id": "09ec68c6",
        "outputId": "624a5b69-74b3-479f-a47c-2419fef7732d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time predict on dev data is approx: 4.98 seconds\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "dev_predictions = getPredictions(dev_df_context, T, E, pi)\n",
        "dev_acc, dev_prec, dev_rec, dev_cnts = HMMaccuracy(dev_predictions, q=3)\n",
        "end = time.time()\n",
        "print(f\"Time predict on dev data is approx: {round((end-start),2)} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a639b81d",
      "metadata": {
        "id": "a639b81d",
        "outputId": "eff04725-14fe-49dd-f4db-4f229309811f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.4106 \n",
            "\n",
            "Precision (H,C,E):\n",
            "\t [0.395  0.6449 0.6048] \n",
            "\n",
            "Recall (H,C,E):\n",
            "\t [0.9844 0.0926 0.0162] \n",
            "\n",
            "Counts (H,C,E) x (H,C,E):\n",
            "[[43878   648    47]\n",
            " [42771  4389   234]\n",
            " [24424  1769   430]]\n"
          ]
        }
      ],
      "source": [
        "print(f\"Accuracy: {round(dev_acc,4)} \\n\")\n",
        "print(f\"Precision (H,C,E):\\n\\t {dev_prec.round(decimals = 4)} \\n\")\n",
        "print(f\"Recall (H,C,E):\\n\\t {dev_rec.round(decimals = 4)} \\n\")\n",
        "print(\"Counts (H,C,E) x (H,C,E):\")\n",
        "print(dev_cnts)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    },
    "colab": {
      "name": "Pre-processing emissions.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}